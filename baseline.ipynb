{"cells":[{"cell_type":"code","execution_count":null,"id":"63c1d209","metadata":{"id":"63c1d209"},"outputs":[],"source":["import os\n","import cv2\n","import sys\n","import matplotlib.pyplot as plt\n","import glob\n","import itertools\n","import random\n","import numpy as np\n","import torch\n","import time\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import torch.nn.functional as F\n","from model import *"]},{"cell_type":"code","execution_count":null,"id":"f5aa57f3","metadata":{"id":"f5aa57f3"},"outputs":[],"source":["batch_size = 3\n","lr=0.0002\n","num_epoch = 20\n","decay_epochs = 9\n","roots = 'data/ukiless'\n","dataname = 'ukiless'"]},{"cell_type":"markdown","id":"51b78f08","metadata":{"id":"51b78f08"},"source":["## helper function for horse2zebra dataset."]},{"cell_type":"code","execution_count":null,"id":"02d3bcc3","metadata":{"id":"02d3bcc3"},"outputs":[],"source":["def remove_images_from_folder(folder):# remove gray image \n","    images = []\n","    for filename in os.listdir(folder):\n","        #print(filename)\n","        img = plt.imread(os.path.join(folder,filename))\n","        images.append(img)\n","        if len(img.shape)!=3:\n","            print(folder+filename,img.shape)\n","            os.remove(folder+'/'+filename)\n","            print('delete W&B image',folder+'/'+filename)\n","    img_size = img.shape[1]\n","    return img_size\n","direc = 'data/horse2zebra/train/B'\n","img_shape = remove_images_from_folder(direc)\n","#print(len(imgs),img_shape)\n","#print([i.shape for i in imgs if i.shape!=(256,256,3)])"]},{"cell_type":"code","execution_count":null,"id":"64fadc3d","metadata":{"id":"64fadc3d"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"b4afc176","metadata":{"id":"b4afc176"},"source":["## Create dataset"]},{"cell_type":"code","execution_count":null,"id":"20bcef44","metadata":{"id":"20bcef44"},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, root, transform=None, unaligned=False, mode=\"train\"):\n","        self.f_A = sorted(glob.glob(os.path.join(root, f\"{mode}/A\") + \"/*.*\"))#include all images\n","        #print(self._A)\n","        self.f_B = sorted(glob.glob(os.path.join(root, f\"{mode}/B\") + \"/*.*\"))\n","        self.transform = transform\n","        self.unaligned = unaligned\n","\n","    def __getitem__(self, index):\n","        A = self.transform(Image.open(self.f_A[index % len(self.f_A)]))\n","        if self.unaligned:\n","            B = self.transform(Image.open(self.f_B[random.randint(0, len(self.f_B) - 1)]))#remove last element\n","        else:\n","            B = self.transform(Image.open(self.f_B[index % len(self.f_B)]))\n","        return {\"A\": A, \"B\": B}# dict for 2 sets\n","\n","    def __len__(self):\n","        return max(len(self.f_A), len(self.f_B))"]},{"cell_type":"code","execution_count":null,"id":"d0d30a61","metadata":{"id":"d0d30a61"},"outputs":[],"source":["dataset = Dataset(root=roots,\n","                       transform=transforms.Compose([\n","                           transforms.Resize(int(img_shape * 1.12), transforms.InterpolationMode.BICUBIC),#random cropping\n","                           transforms.RandomCrop(img_shape),\n","                           transforms.RandomHorizontalFlip(),\n","                           transforms.ToTensor(),\n","                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))#normalized in RGB channel\n","                       ]),\n","                           unaligned=True)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"id":"60e1625c","metadata":{"id":"60e1625c"},"outputs":[],"source":["random.seed(random.randint(1, 10000))\n","torch.manual_seed(random.randint(1, 10000))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.empty_cache()\n","print(device)"]},{"cell_type":"code","execution_count":null,"id":"38b8a149","metadata":{"id":"38b8a149"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"95c2b259","metadata":{"id":"95c2b259"},"outputs":[],"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm\") != -1:\n","        torch.nn.init.zeros_(m.bias)#initialize bias\n","        torch.nn.init.normal_(m.weight, 1.0, 0.02)#set  mean and variance of Gaussian \n","    \n"]},{"cell_type":"code","execution_count":null,"id":"e33f0cbf","metadata":{"scrolled":true,"id":"e33f0cbf"},"outputs":[],"source":["netG_A2B = Generator().to(device)\n","netG_B2A = Generator().to(device)\n","netD_A = Discriminator().to(device)\n","netD_B = Discriminator().to(device)\n","\n","netG_A2B.apply(weights_init)#self define initialize\n","netG_B2A.apply(weights_init)\n","netD_A.apply(weights_init)\n","netD_B.apply(weights_init)\n","print(\"init finish\")"]},{"cell_type":"code","execution_count":null,"id":"627e8339","metadata":{"id":"627e8339"},"outputs":[],"source":["cycle_loss = torch.nn.L1Loss().to(device)#L1 loss\n","iden_loss = torch.nn.L1Loss().to(device)\n","adv_loss = torch.nn.MSELoss().to(device)\n","optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n","lr=lr, betas=(0.5, 0.999))\n","optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=lr, betas=(0.5, 0.999))\n","optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=lr, betas=(0.5, 0.999))\n","print('Optimizer and loss')"]},{"cell_type":"code","execution_count":null,"id":"287a438d","metadata":{"id":"287a438d"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"6d9b9c60","metadata":{"id":"6d9b9c60"},"outputs":[],"source":["class DecayLR:#Linear decay lr after some point from 2e-4 to 2e-6(almost 0)\n","    def __init__(self, epochs, offset, decay_epochs):\n","        epoch_flag = epochs - decay_epochs\n","        assert (epoch_flag > 0), \"decay should start later\"\n","        self.epochs = epochs\n","        self.offset = offset\n","        self.decay_epochs = decay_epochs\n","\n","    def step(self, epoch):\n","        return 1.0 - max(0, epoch  - self.decay_epochs) / (\n","                self.epochs - self.decay_epochs)\n"]},{"cell_type":"code","execution_count":null,"id":"4f4233d2","metadata":{"id":"4f4233d2"},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, max_size=50):\n","        assert (max_size > 0), \"must >0.\"\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:#single image\n","            element = torch.unsqueeze(element, 0)\n","            if len(self.data) < self.max_size:\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0, 1) > 0.5:# half probability\n","                    i = random.randint(0, self.max_size - 1)\n","                    to_return.append(self.data[i].clone())#Bootstrapping\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return torch.cat(to_return)"]},{"cell_type":"code","execution_count":null,"id":"7986641b","metadata":{"id":"7986641b"},"outputs":[],"source":["lr_lambda = DecayLR(num_epoch, 0, decay_epochs).step\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lr_lambda)# linear lr decrease after some point\n","lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lr_lambda)\n","lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lr_lambda)"]},{"cell_type":"code","execution_count":null,"id":"a40ff329","metadata":{"id":"a40ff329"},"outputs":[],"source":["g_losses = []\n","d_losses = []\n","identity_losses = []\n","gan_losses = []\n","cycle_losses = []\n","fake_A_buffer = ReplayBuffer()\n","fake_B_buffer = ReplayBuffer()"]},{"cell_type":"code","execution_count":null,"id":"acaa9183","metadata":{"id":"acaa9183"},"outputs":[],"source":["for epoch in range(num_epoch):\n","    t1 = time.time()\n","    progress = tqdm(enumerate(dataloader))\n","    for idx,imgs in progress:\n","        #Get data\n","        real_A = imgs[\"A\"].to(device)\n","        real_B = imgs[\"B\"].to(device)\n","        batch_size_real = real_A.shape[0]\n","        real_label = torch.full((batch_size_real, 1), 1, device=device, dtype=torch.float32)\n","        fake_label = torch.full((batch_size_real, 1), 0, device=device, dtype=torch.float32)\n","        #######Set generator\n","\n","        # Set G_A and G_B's gradients to zero\n","        optimizer_G.zero_grad()\n","\n","        # Identity loss\n","        # G_B2A(A) should equal A if real A is fed\n","        identity_image_A = netG_B2A(real_A)\n","        loss_identity_A = iden_loss(identity_image_A, real_A) * 5.0 #set weight\n","        # G_A2B(B) should equal B if real B is fed\n","        #print('!'*20,real_image_B.shape)\n","        identity_image_B = netG_A2B(real_B)\n","        loss_identity_B = iden_loss(identity_image_B, real_B) * 5.0\n","\n","        # GAN loss\n","        # GAN loss D_A(G_A(A))\n","        fake_image_A = netG_B2A(real_B)\n","        fake_output_A = netD_A(fake_image_A)\n","        loss_GAN_B2A = adv_loss(fake_output_A, real_label)\n","        # GAN loss D_B(G_B(B))\n","        fake_image_B = netG_A2B(real_A)\n","        fake_output_B = netD_B(fake_image_B)\n","        loss_GAN_A2B = adv_loss(fake_output_B, real_label)\n","\n","        # Cycle loss\n","        recovered_image_A = netG_B2A(fake_image_B)\n","        loss_cycle_ABA = cycle_loss(recovered_image_A, real_A) * 10.0\n","\n","        recovered_image_B = netG_A2B(fake_image_A)\n","        loss_cycle_BAB = cycle_loss(recovered_image_B, real_B) * 10.0\n","\n","        # Combined loss and calculate gradients\n","        errG = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n","\n","        # Calculate gradients for G_A and G_B\n","        errG.backward()\n","        # Update G_A and G_B's weights\n","        optimizer_G.step()\n","        #####Set discriminator  A\n","\n","        # Set D_A gradients to zero\n","        optimizer_D_A.zero_grad()\n","\n","        # Real A image loss\n","        real_output_A = netD_A(real_A)\n","        errD_real_A = adv_loss(real_output_A, real_label)\n","\n","        # Fake A image loss\n","        fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n","        fake_output_A = netD_A(fake_image_A.detach())\n","        errD_fake_A = adv_loss(fake_output_A, fake_label)\n","\n","        # Combined loss and calculate gradients\n","        errD_A = (errD_real_A + errD_fake_A) / 2\n","\n","        # Calculate gradients for D_A\n","        errD_A.backward()\n","        # Update D_A weights\n","        optimizer_D_A.step()\n","\n","        #######Discrimintor B\n","\n","        # Set D_B gradients to zero\n","        optimizer_D_B.zero_grad()\n","\n","        # Real B image loss\n","        real_output_B = netD_B(real_B)\n","        errD_real_B = adv_loss(real_output_B, real_label)\n","\n","        # Fake B image loss\n","        fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n","        fake_output_B = netD_B(fake_image_B.detach())\n","        errD_fake_B = adv_loss(fake_output_B, fake_label)\n","\n","        # Combined loss and calculate gradients\n","        errD_B = (errD_real_B + errD_fake_B) / 2\n","\n","        # Calculate gradients for D_B\n","        errD_B.backward()\n","        # Update D_B weights\n","        optimizer_D_B.step()\n","        with torch.no_grad():\n","            d_losses += [(errD_A + errD_B).item()]\n","            g_losses += [errG.item()]\n","            identity_losses += [(loss_identity_A + loss_identity_B).item()]\n","            gan_losses += [(loss_GAN_A2B + loss_GAN_B2A).item()]\n","            cycle_losses += [(loss_cycle_ABA + loss_cycle_BAB).item()]\n","        progress.set_description(\n","            f\"[{epoch}/{num_epoch - 1}][{idx}/{len(dataloader) - 1}] \"\n","            f\"Loss_D: {(errD_A + errD_B).item():.4f} \"\n","            f\"Loss_G: {errG.item():.4f} \"\n","            f\"Loss_G_identity: {(loss_identity_A + loss_identity_B).item():.4f} \"\n","            f\"loss_G_GAN: {(loss_GAN_A2B + loss_GAN_B2A).item():.4f} \"\n","            f\"loss_G_cycle: {(loss_cycle_ABA + loss_cycle_BAB).item():.4f}\"\n","            f'time{time.time()-t1}')\n","            \n","    # save model\n","    if epoch%10 == 10:\n","        torch.save(netG_A2B.state_dict(), f\"weights/{dataname}/netG_A2B_epoch_{epoch}.pth\")\n","        torch.save(netG_B2A.state_dict(), f\"weights/{dataname}/netG_B2A_epoch_{epoch}.pth\")\n","        torch.save(netD_A.state_dict(), f\"weights/{dataname}/netD_A_epoch_{epoch}.pth\")\n","        torch.save(netD_B.state_dict(), f\"weights/{dataname}/netD_B_epoch_{epoch}.pth\")\n","# Update learning rates\n","    lr_scheduler_G.step()\n","    lr_scheduler_D_A.step()\n","    lr_scheduler_D_B.step()\n","np.save('g_losses.npy',g_losses)\n","np.save('d_losses.npy',d_losses)\n","np.save('identity_losses.npy',identity_losses)\n","np.save('gan_losses.npy',gan_losses)\n","np.save('cycle_losses.npy',cycle_losses)\n","# save last check pointing\n","torch.save(netG_A2B.state_dict(), f\"weights/{dataname}/netG_A2B.pth\")\n","torch.save(netG_B2A.state_dict(), f\"weights/{dataname}/netG_B2A.pth\")\n","torch.save(netD_A.state_dict(), f\"weights/{dataname}/netD_A.pth\")\n","torch.save(netD_B.state_dict(), f\"weights/{dataname}/netD_B.pth\")\n"]},{"cell_type":"code","execution_count":null,"id":"15cd10f7","metadata":{"id":"15cd10f7"},"outputs":[],"source":["dataset = 'uki'\n","torch.save(netG_A2B.state_dict(), f\"weights/{dataset}/netG_A2B.pth\")\n","torch.save(netG_B2A.state_dict(), f\"weights/{dataset}/netG_B2A.pth\")\n","torch.save(netD_A.state_dict(), f\"weights/{dataset}/netD_A.pth\")\n","torch.save(netD_B.state_dict(), f\"weights/{dataset}/netD_B.pth\")\n","plt.plot(g_losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"b03e892a","metadata":{"id":"b03e892a"},"outputs":[],"source":["plt.plot(d_losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"0d35a512","metadata":{"id":"0d35a512"},"outputs":[],"source":["plt.plot(identity_losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1515ad4b","metadata":{"id":"1515ad4b"},"outputs":[],"source":["plt.plot(gan_losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"39f3a40f","metadata":{"id":"39f3a40f"},"outputs":[],"source":["plt.plot(cycle_losses)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"6cc9c548","metadata":{"id":"6cc9c548"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (clean)","language":"python","name":"python3_clean"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"baseline.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}